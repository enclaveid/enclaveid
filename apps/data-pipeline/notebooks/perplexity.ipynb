{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from llama_cpp.llama_chat_format import format_llama3\n",
    "from huggingface_hub import hf_hub_download\n",
    "import llama_cpp\n",
    "\n",
    "\n",
    "filename = \"llama-3.2-1b-instruct-q8_0.gguf\"\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"hugging-quants/Llama-3.2-1B-Instruct-Q8_0-GGUF\",\n",
    "    filename=filename,\n",
    "    local_dir=\".\",  # Download to current directory\n",
    "    local_dir_use_symlinks=False  # Get actual file instead of symlink\n",
    ")\n",
    "\n",
    "llm = llama_cpp.Llama(\n",
    "            model_path=model_path,\n",
    "            n_ctx=2048,   # Context window size\n",
    "            n_batch=512,  # Batch size for prompt processing\n",
    "            logits_all=True,  # <-- IMPORTANT: Collect all logits\n",
    "        )\n",
    "\n",
    "\n",
    "def get_perplexity(text: str):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant, trying to guess the context behind single messages from a chat app.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": text\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    chat = format_llama3(messages)\n",
    "    prompt = chat.prompt + chat.stop\n",
    "\n",
    "\n",
    "    llm.reset() # important to reset the model state before each run\n",
    "        \n",
    "    # Encode the text to tokens\n",
    "    tokens = llm.tokenize(prompt.encode())\n",
    "    llm.eval(tokens)\n",
    "    logits = np.array(llm.eval_logits)\n",
    "    logprobs = llm.logits_to_logprobs(logits)\n",
    "\n",
    "    # Skip the first token (there is no \"previous\" context for it)\n",
    "    selected_logprobs = []\n",
    "    for i in range(1, len(tokens)):\n",
    "        token_id = tokens[i]\n",
    "        selected_logprobs.append(logprobs[i-1, token_id])\n",
    "\n",
    "\n",
    "    # Calculate metrics\n",
    "    cross_entropy = -sum(selected_logprobs) / len(selected_logprobs)\n",
    "    perplexity = math.exp(cross_entropy)\n",
    "\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_perplexity(\"I’ll have friends for dinner tonight if it’s not a problem for u  If u want to join us feel free :)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}