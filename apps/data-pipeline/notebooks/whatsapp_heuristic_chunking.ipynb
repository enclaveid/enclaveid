{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from dtale import show\n",
    "import dtale.global_state as global_state\n",
    "\n",
    "global_state.set_app_settings(dict(max_column_width=300))\n",
    "\n",
    "msg = \"/Users/ma9o/Downloads/latest 2/Estela.json\"\n",
    "\n",
    "df = pl.read_json(msg).with_columns(pl.col(\"datetime\").str.strptime(pl.Datetime).alias(\"datetime\")).sort(\"datetime\")\n",
    "\n",
    "show(df.to_pandas()).open_browser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    # Detect whenever role or content changes from the previous row\n",
    "    df.with_columns(\n",
    "        (\n",
    "            (pl.col(\"role\") != pl.col(\"role\").shift()) | \n",
    "            (pl.col(\"content\") != pl.col(\"content\").shift())\n",
    "        )  # This will be True at each boundary between consecutive message groups\n",
    "        .cast(pl.Int8)  # Convert boolean to integer 0/1\n",
    "        .cum_sum()       # Cumulative sum will create a unique 'group_id' for each run of consecutive duplicates\n",
    "        .alias(\"group_id\")\n",
    "    )\n",
    "    # Now group by role, content, and group_id\n",
    "    .group_by([\"group_id\", \"role\", \"content\"])\n",
    "    .agg([\n",
    "        pl.col(\"datetime\").min().alias(\"datetime\"),\n",
    "        pl.len().alias(\"message_count\")\n",
    "    ])\n",
    "    .drop(\"group_id\")\n",
    "    .sort(\"datetime\")\n",
    ")\n",
    "\n",
    "show(df.to_pandas()).open_browser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group messages by hour and count them\n",
    "hourly_messages = (\n",
    "    df\n",
    "    .with_columns(pl.col(\"datetime\").str.strptime(pl.Datetime).alias(\"datetime\"))  # Convert string to datetime\n",
    "    .with_columns(pl.col(\"datetime\").dt.truncate(\"1h\").alias(\"hour\"))\n",
    "    .filter(pl.col(\"hour\").gt(pl.datetime(2025, 1, 1)))\n",
    "    .sort(\"hour\")\n",
    "    .group_by(\"hour\")\n",
    "    .agg(pl.col(\"message_count\").sum().alias(\"total_messages\"))\n",
    "    # Create a complete sequence of hours and fill missing values with 0\n",
    "    .upsample(\n",
    "        time_column=\"hour\",\n",
    "        every=\"1h\"\n",
    "    )\n",
    "    .fill_null(0)\n",
    "    .sort(\"hour\")\n",
    ")\n",
    "\n",
    "# Create an interactive plot using dtale\n",
    "show(hourly_messages.to_pandas()).open_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from ruptures import Pelt\n",
    "import time\n",
    "\n",
    "def add_chat_sessions(df: pl.DataFrame) -> pl.DataFrame:\n",
    "\n",
    "    base_time = df.with_columns([\n",
    "        pl.col('datetime').dt.epoch(time_unit='s')\n",
    "    ]).get_column('datetime').min()\n",
    "\n",
    "    \n",
    "    # Convert datetime to seconds since start\n",
    "    seconds = df.with_columns([\n",
    "        pl.col('datetime').dt.epoch(time_unit='s') - pl.lit(base_time)\n",
    "    ]).get_column('datetime').to_numpy()\n",
    "\n",
    "    # Create signal combining time gaps and message counts\n",
    "    message_counts = df['message_count'].to_numpy()\n",
    "    \n",
    "    # Combine into 2D signal - time gaps between messages and message counts\n",
    "    time_gaps = np.diff(seconds, prepend=seconds[0])\n",
    "    signal = np.column_stack((time_gaps, message_counts))\n",
    "\n",
    "    print(\"Running PELT\")\n",
    "    start = time.time()\n",
    "    \n",
    "    # Apply PELT\n",
    "    algo = Pelt(model=\"rbf\", min_size=3, jump=1).fit(signal)\n",
    "    change_points = algo.predict(pen=np.log(len(signal)))\n",
    "\n",
    "    print(f\"PELT took {time.time() - start} seconds\")\n",
    "\n",
    "    print(change_points)\n",
    "    \n",
    "    # Create session labels\n",
    "    session_labels = np.zeros(len(df), dtype=int)\n",
    "    current_session = 0\n",
    "    \n",
    "    for cp in change_points[:-1]:  # Last point is always signal length\n",
    "        session_labels[cp:] = current_session + 1\n",
    "        current_session += 1\n",
    "    \n",
    "    # Add session labels to DataFrame\n",
    "    return df.with_columns(\n",
    "        session=pl.Series(session_labels)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = add_chat_sessions(df)\n",
    "show(df2.to_pandas()).open_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "from hdbscan.prediction import all_points_membership_vectors\n",
    "\n",
    "\n",
    "def cluster_messages(time_dim: np.ndarray, density_dim: np.ndarray) -> pl.Series:\n",
    "    data = np.column_stack((time_dim, density_dim))\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=5, metric='euclidean', prediction_data=True).fit(data)\n",
    "\n",
    "    membership_vectors = all_points_membership_vectors(clusterer)\n",
    "\n",
    "    fine_cluster_labels_soft = membership_vectors.argmax(axis=1)\n",
    "\n",
    "    return pl.Series(fine_cluster_labels_soft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from llama_cpp.llama_chat_format import format_llama3\n",
    "from huggingface_hub import hf_hub_download\n",
    "import llama_cpp\n",
    "\n",
    "\n",
    "filename = \"llama-3.2-1b-instruct-q8_0.gguf\"\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"hugging-quants/Llama-3.2-1B-Instruct-Q8_0-GGUF\",\n",
    "    filename=filename,\n",
    "    local_dir=\".\",  # Download to current directory\n",
    "    local_dir_use_symlinks=False  # Get actual file instead of symlink\n",
    ")\n",
    "\n",
    "llm = llama_cpp.Llama(\n",
    "            model_path=model_path,\n",
    "            n_ctx=2048,   # Context window size\n",
    "            n_batch=512,  # Batch size for prompt processing\n",
    "            logits_all=True,  # <-- IMPORTANT: Collect all logits\n",
    "            n_threads=8,\n",
    "            n_gpu_layers=-1,\n",
    "        )\n",
    "\n",
    "\n",
    "def get_perplexity(text: str):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant, trying to guess the context behind single messages from a chat app.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": text\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    chat = format_llama3(messages)\n",
    "    prompt = chat.prompt + chat.stop\n",
    "\n",
    "\n",
    "    llm.reset() # important to reset the model state before each run\n",
    "        \n",
    "    # Encode the text to tokens\n",
    "    tokens = llm.tokenize(prompt.encode())\n",
    "    llm.eval(tokens)\n",
    "    logits = np.array(llm.eval_logits)\n",
    "    logprobs = llm.logits_to_logprobs(logits)\n",
    "\n",
    "    # Skip the first token (there is no \"previous\" context for it)\n",
    "    selected_logprobs = []\n",
    "    for i in range(1, len(tokens)):\n",
    "        token_id = tokens[i]\n",
    "        selected_logprobs.append(logprobs[i-1, token_id])\n",
    "\n",
    "\n",
    "    # Calculate metrics\n",
    "    cross_entropy = -sum(selected_logprobs) / len(selected_logprobs)\n",
    "    perplexity = math.exp(cross_entropy)\n",
    "\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# First, calculate perplexities with progress bar\n",
    "perplexities = []\n",
    "for content in tqdm(df['content'], desc=\"Calculating perplexities\"):\n",
    "    perplexities.append(get_perplexity(content))\n",
    "\n",
    "# Then, add them to the dataframe\n",
    "df = df.with_columns(\n",
    "    perplexity=pl.Series(perplexities)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "def compression_ratio(text):\n",
    "    text_bytes = text.encode('utf-8')\n",
    "    compressed = gzip.compress(text_bytes)\n",
    "    return len(compressed) / len(text_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df.with_columns(\n",
    "    compression=pl.col('content').map_elements(compression_ratio, return_dtype=pl.Float64)\n",
    ").with_columns(\n",
    "    clusters=cluster_messages(df4['datetime'].dt.timestamp().to_numpy(), df4['compression'].to_numpy())\n",
    ")\n",
    "\n",
    "show(df4.to_pandas()).open_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}