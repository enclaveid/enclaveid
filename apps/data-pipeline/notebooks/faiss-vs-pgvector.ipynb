{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "import psycopg2\n",
    "import time\n",
    "from psycopg2.extras import execute_values\n",
    "import pandas as pd\n",
    "from typing import Tuple, List\n",
    "\n",
    "def create_random_vectors(num_vectors=100_000, dim=4096):\n",
    "    vectors = np.random.randn(num_vectors, dim).astype('float32')\n",
    "    vectors = vectors / np.linalg.norm(vectors, axis=1)[:, np.newaxis]\n",
    "    return vectors\n",
    "\n",
    "def setup_pgvector_db():\n",
    "    conn = psycopg2.connect(\"dbname=postgres\")\n",
    "    conn.autocommit = True\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    cur.execute(\"DROP DATABASE IF EXISTS vector_benchmark;\")\n",
    "    cur.execute(\"CREATE DATABASE vector_benchmark;\")\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    \n",
    "    conn = psycopg2.connect(\"\"\"\n",
    "        dbname=vector_benchmark \n",
    "        options='-c maintenance_work_mem=2GB \n",
    "                -c synchronous_commit=off '\n",
    "    \"\"\")\n",
    "    \n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SET work_mem = '1GB';\")\n",
    "    cur.execute(\"SET maintenance_work_mem = '2GB';\")\n",
    "    cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n",
    "    \n",
    "    cur.execute(\"\"\"\n",
    "        CREATE TABLE items (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            embedding vector(4096)\n",
    "        );\n",
    "    \"\"\")\n",
    "    \n",
    "    cur.execute(\"ALTER TABLE items SET (autovacuum_enabled = false);\")\n",
    "    conn.commit()\n",
    "    return conn, cur\n",
    "\n",
    "def fetch_vectors_from_db(cur) -> np.ndarray:\n",
    "    \"\"\"Fetch all vectors from database efficiently\"\"\"\n",
    "    print(\"Fetching vectors from database...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Fetch in batches to manage memory\n",
    "    batch_size = 10000\n",
    "    vectors = []\n",
    "    \n",
    "    cur.execute(\"SELECT COUNT(*) FROM items\")\n",
    "    total_rows = cur.fetchone()[0]\n",
    "    \n",
    "    for offset in range(0, total_rows, batch_size):\n",
    "        cur.execute(\n",
    "            \"SELECT embedding FROM items ORDER BY id LIMIT %s OFFSET %s\",\n",
    "            (batch_size, offset)\n",
    "        )\n",
    "        batch = cur.fetchall()\n",
    "        vectors.extend([np.array(row[0]) for row in batch])\n",
    "    \n",
    "    vectors_array = np.vstack(vectors).astype('float32')\n",
    "    \n",
    "    print(f\"Fetching vectors took {time.time() - start_time:.2f} seconds\")\n",
    "    return vectors_array\n",
    "\n",
    "def insert_vectors_to_postgres(vectors, cur, conn, batch_size=5000):\n",
    "    print(\"Inserting vectors to PostgreSQL...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    data = [(vector.tolist(),) for vector in vectors]\n",
    "    cur.execute(\"BEGIN;\")\n",
    "    \n",
    "    execute_values(\n",
    "        cur,\n",
    "        \"INSERT INTO items (embedding) VALUES %s\",\n",
    "        data,\n",
    "        template=\"(%s::vector)\",\n",
    "        page_size=batch_size\n",
    "    )\n",
    "    \n",
    "    conn.commit()\n",
    "    cur.execute(\"ALTER TABLE items SET (autovacuum_enabled = true);\")\n",
    "    conn.commit()\n",
    "    \n",
    "    print(f\"Insertion took {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "def create_pgvector_index(cur, conn):\n",
    "    print(\"Creating pgvector index...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    cur.execute(\"SET maintenance_work_mem = '2GB';\")\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE INDEX ON items USING ivfflat (embedding vector_cosine_ops) \n",
    "        WITH (lists = 1000);\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    print(f\"Index creation took {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "def benchmark_pgvector(vectors, cur, num_queries=100, k=20) -> Tuple[pd.Series, float]:\n",
    "    print(f\"\\nTesting pgvector with {num_queries} queries...\")\n",
    "    query_vectors = vectors[:num_queries]\n",
    "    \n",
    "    times = []\n",
    "    start_total = time.time()\n",
    "    \n",
    "    for query in query_vectors:\n",
    "        start = time.time()\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT id, embedding <=> %s as distance \n",
    "            FROM items \n",
    "            ORDER BY embedding <=> %s \n",
    "            LIMIT %s\n",
    "        \"\"\", (query.tolist(), query.tolist(), k))\n",
    "        results = cur.fetchall()\n",
    "        times.append(time.time() - start)\n",
    "    \n",
    "    total_time = time.time() - start_total\n",
    "    return pd.Series(times).describe(), total_time\n",
    "\n",
    "def benchmark_faiss_with_loading(cur, num_queries=100, k=20) -> Tuple[pd.Series, float, dict]:\n",
    "    print(f\"\\nTesting FAISS (including data loading) with {num_queries} queries...\")\n",
    "    timings = {}\n",
    "    \n",
    "    # Time vector loading\n",
    "    start_load = time.time()\n",
    "    vectors = fetch_vectors_from_db(cur)\n",
    "    load_time = time.time() - start_load\n",
    "    timings['load_time'] = load_time\n",
    "    \n",
    "    # Time index building\n",
    "    start_build = time.time()\n",
    "    index = faiss.IndexFlatL2(vectors.shape[1])\n",
    "    index.add(vectors)\n",
    "    build_time = time.time() - start_build\n",
    "    timings['build_time'] = build_time\n",
    "    \n",
    "    # Time queries\n",
    "    query_vectors = vectors[:num_queries]\n",
    "    times = []\n",
    "    start_total = time.time()\n",
    "    \n",
    "    for query in query_vectors:\n",
    "        start = time.time()\n",
    "        D, I = index.search(query.reshape(1, -1), k)\n",
    "        times.append(time.time() - start)\n",
    "    \n",
    "    query_time = time.time() - start_total\n",
    "    timings['total_query_time'] = query_time\n",
    "    \n",
    "    return pd.Series(times).describe(), sum(timings.values()), timings\n",
    "\n",
    "def run_benchmark():\n",
    "    print(\"Generating random vectors...\")\n",
    "    vectors = create_random_vectors()\n",
    "    \n",
    "    # Setup and populate PostgreSQL\n",
    "    conn, cur = setup_pgvector_db()\n",
    "    insert_vectors_to_postgres(vectors, cur, conn)\n",
    "    create_pgvector_index(cur, conn)\n",
    "    \n",
    "    # Run benchmarks\n",
    "    pg_stats, pg_total = benchmark_pgvector(vectors, cur)\n",
    "    faiss_stats, faiss_total, faiss_timings = benchmark_faiss_with_loading(cur)\n",
    "    \n",
    "    print(\"\\n=== Results ===\")\n",
    "    print(\"\\npgvector query times (seconds):\")\n",
    "    print(pg_stats)\n",
    "    print(f\"Total time for pgvector: {pg_total:.2f} seconds\")\n",
    "    \n",
    "    print(\"\\nFAISS timings (seconds):\")\n",
    "    print(\"Loading data:\", faiss_timings['load_time'])\n",
    "    print(\"Building index:\", faiss_timings['build_time'])\n",
    "    print(\"Query execution:\", faiss_timings['total_query_time'])\n",
    "    print(\"\\nFAISS query statistics (seconds):\")\n",
    "    print(faiss_stats)\n",
    "    print(f\"Total time for FAISS (including all overhead): {faiss_total:.2f} seconds\")\n",
    "    \n",
    "    # Cleanup\n",
    "    cur.execute(\"DROP INDEX items_embedding_idx;\")\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    \n",
    "    conn = psycopg2.connect(\"dbname=postgres\")\n",
    "    conn.autocommit = True\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"DROP DATABASE vector_benchmark;\")\n",
    "    cur.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}